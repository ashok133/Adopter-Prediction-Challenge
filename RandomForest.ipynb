{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RandomForest.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"V1rh8exUxxpk","colab_type":"text"},"cell_type":"markdown","source":["\n","## **Adopter Prediction Challenge**\n","\n"," ~ Ankita, Ashok, Kaydee, Young\n"," \n"," ---\n","\n","Website XYZ, a music-listening social networking website, follows the “freemium” business model. The website offers basic services for free, and provides a number of additional premium capabilities for a monthly subscription fee. We are interested in predicting which people would be likely to convert from free users to premium subscribers in the next 6 month period, if they are targeted by our promotional campaign.\n","\n","### Dataset\n","\n","We have a dataset from the previous marketing campaign which targeted a number of non-subscribers.\n","\n","Features: \n","\n","```\n","1.   adopter (predictor class)\n","2.   user_id\n","3.   age\n","4.   male\n","5.   friend_cnt\n","6.   avg_friend_age\n","7.   avg_friend_male\n","8.   friend_country_cnt\n","9.   subscriber_friend_cnt\n","10.   songsListened\n","11.   lovedTracks\n","12.   posts\n","13.   playlists\n","14.   shouts\n","15.   good_country\n","16.   tenure\n","17.   *other delta variables*\n","```\n","\n","\n","\n","### Task\n","\n","The task is to build the best predictive model for the next marketing campaign, i.e., for predicting likely `adopters` (that is, which current non- subscribers are likely to respond to the marketing campaign and sign up for the premium service within 6 months after the campaign).\n","\n","---"]},{"metadata":{"id":"7BFSsGGrrAWo","colab_type":"code","outputId":"5e66ec9c-5d40-4a29-93f9-26e99bb74ff6","executionInfo":{"status":"ok","timestamp":1554867182047,"user_tz":240,"elapsed":3847,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["!pip3 install sklearn"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.20.3)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.1.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.14.6)\n"],"name":"stdout"}]},{"metadata":{"id":"z5jYBuZXxrl-","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.utils import shuffle\n","from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, classification_report, recall_score, f1_score, accuracy_score, precision_score\n","from sklearn.ensemble import RandomForestClassifier\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qbw2hXELhkxW","colab_type":"code","colab":{}},"cell_type":"code","source":["# setting fixed seed value for consistency in results\n","seed = 7\n","np.random.seed(seed)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"biD6vPF8wkO_","colab_type":"code","outputId":"7dae1e22-1189-4969-815f-1d94e944950a","executionInfo":{"status":"ok","timestamp":1555124491925,"user_tz":240,"elapsed":1605,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["# original dataset\n","data = pd.read_csv('https://drive.google.com/uc?export=view&id=1wctM0dYDj839zp6sTlFnDgCmFspXhDuW')\n","\n","# data.columns\n","\n","data.adopter.value_counts()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    85142\n","1     1540\n","Name: adopter, dtype: int64"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"l8_QqI5UiMXZ","colab_type":"text"},"cell_type":"markdown","source":["Checking to see if any features (especially adopter) needs to be encoded as int"]},{"metadata":{"id":"P0wxoC7gWIRy","colab_type":"code","outputId":"68045b79-aee1-4b87-ed76-a44dda4487e1","executionInfo":{"status":"ok","timestamp":1555124498044,"user_tz":240,"elapsed":182,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# some housekeeping for metrics\n","recalls = {}\n","f1s = {}\n","precisions = {}\n","accuracies = {}\n","\n","# splitting original dataset into features and predictor\n","X = data.iloc[:, data.columns != 'adopter']\n","y = data.iloc[:, data.columns == 'adopter']\n","\n","# splitting the original dataset for cross-validation (0.7 train, 0.3 test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","\n","print (\"Number of train instances: {}\".format(len(X_train)))\n","print (\"Number of test instances: {}\".format(len(X_test)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Number of train instances: 60677\n","Number of test instances: 26005\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"m4-LfIKOVjBN"},"cell_type":"markdown","source":["## SMOTE splitting\n","\n","We'll use SMOTE (Synthetic Minority Oversampling Technique) to create(synthesize) more samples of minority class. The recall score we got earlier might be less as we imputed more than 80% of the data to balance the dataset. "]},{"metadata":{"id":"vcEGpOHQAcx_","colab_type":"text"},"cell_type":"markdown","source":["Before we SMOTE the entire dataset, synthesizing around 58000 new instances of minority will not introduce enough variation in data for the models to learn. \n","\n","We decide that we will include only a subset of the majority class instances (4000) and synthsize 4000-1540=2460 new instances for minority class using SMOTE. That'll (hopefully) avoid our models from overfitting. "]},{"metadata":{"id":"-T7TUPGcBDEL","colab_type":"code","outputId":"40a670c4-9093-4926-b09e-fbcdede9d56d","executionInfo":{"status":"ok","timestamp":1555124501524,"user_tz":240,"elapsed":199,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["# fetching the indices of minority instances\n","adopting_indices = np.array(data[data.adopter == 1].index)\n","\n","# fetching indices of normal instances\n","non_adopting_indices = data[data.adopter == 0].index\n","\n","# randomly select 1540 normal instances to create a partitioned balanced dataset\n","random_non_adopting_indices = np.random.choice(non_adopting_indices,\n","                                            6000,\n","                                            replace = False)\n","random_non_adopting_indices = np.array(random_non_adopting_indices)\n","\n","# combining both the instance groups (minority and the new random set) \n","undersampled_indices = np.concatenate([adopting_indices, random_non_adopting_indices])\n","\n","# creating the undersampled dataset\n","undersampled_data = data.iloc[undersampled_indices, :]\n","\n","# shuffling the set\n","undersampled_data = shuffle(undersampled_data)\n","\n","# storing the features(X) and predictor class(y)\n","X_undersample = undersampled_data.iloc[:, undersampled_data.columns != 'adopter']\n","y_undersample = undersampled_data.iloc[:, undersampled_data.columns == 'adopter']\n","\n","print(\"Number of minority instances: {}\\nNumber of normal instances: {} \\nTotal: {}\".format(len(undersampled_data[undersampled_data.adopter == 1]), \n","                                                                                           len(undersampled_data[undersampled_data.adopter == 0]),\n","                                                                                           len(undersampled_data)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of minority instances: 1540\n","Number of normal instances: 6000 \n","Total: 7540\n"],"name":"stdout"}]},{"metadata":{"id":"3oiBRdANBXfN","colab_type":"code","outputId":"e1bc35c0-5a1e-4321-f9ce-62da34c9fa45","executionInfo":{"status":"ok","timestamp":1555124505190,"user_tz":240,"elapsed":193,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["# splitting original dataset into features and predictor\n","X = undersampled_data.iloc[:, data.columns != 'adopter']\n","y = undersampled_data.iloc[:, data.columns == 'adopter']\n","\n","# splitting the original dataset for cross-validation (0.7 train, 0.3 test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","\n","print (\"Undersampled Data:\")\n","print (\"Number of train instances: {}\".format(len(X_train)))\n","print (\"Number of test instances: {}\".format(len(X_test)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Undersampled Data:\n","Number of train instances: 5278\n","Number of test instances: 2262\n"],"name":"stdout"}]},{"metadata":{"id":"pe2cR4TFOnWB","colab_type":"code","colab":{}},"cell_type":"code","source":["# sm = SMOTE(random_state = 12, ratio = None)\n","# X_train_smoted_np, y_train_smoted_np = sm.fit_sample(X_train, y_train)\n","# # X_train_smoted, y_train_smoted = sm.fit_sample(X_train, y_train.values.ravel())\n","# print(type(X_train_smoted_np))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1JZ7DMhwQ3s1","colab_type":"code","colab":{}},"cell_type":"code","source":["# # checking the lengths of new training set\n","\n","# print (\"Number of SMOTEd instances: {}\".format(len(X_train_smoted_np)))\n","\n","# X_train.head()\n","# y_train_smoted_non_adopters = y_train_smoted_np[y_train_smoted_np == 1]\n","# y_train_smoted_adopters = y_train_smoted_np[y_train_smoted_np == 0]\n","\n","# print (\"Number of SMOTEd non-adopters (adopter = 0): {}\".format(len(y_train_smoted_non_adopters)))\n","# print (\"Number of SMOTEd adopters (adopter = 1): {}\".format(len(y_train_smoted_adopters)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OQR9mY4pUvEF","colab_type":"text"},"cell_type":"markdown","source":["We now have around 2792 instances each of both the classes, which is better than simple undersampling and having only 3080 instances in all.\n","\n"]},{"metadata":{"id":"6dPpzJElwkIS","colab_type":"text"},"cell_type":"markdown","source":["For now we'll import smoted data from our R scripts since the above is taking time."]},{"metadata":{"id":"Lro-UcZ7yTtv","colab_type":"text"},"cell_type":"markdown","source":["## Random Forest with all the features"]},{"metadata":{"id":"PaMPodRehhW9","colab_type":"code","outputId":"26af6eba-38c6-4359-c19b-a4eb9869f833","executionInfo":{"status":"ok","timestamp":1555124511569,"user_tz":240,"elapsed":273,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"cell_type":"code","source":["classifier = RandomForestClassifier(random_state=0, n_jobs=-1, class_weight=\"balanced\")\n","\n","classifier.fit(X_train, y_train)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight='balanced',\n","            criterion='gini', max_depth=None, max_features='auto',\n","            max_leaf_nodes=None, min_impurity_decrease=0.0,\n","            min_impurity_split=None, min_samples_leaf=1,\n","            min_samples_split=2, min_weight_fraction_leaf=0.0,\n","            n_estimators=10, n_jobs=-1, oob_score=False, random_state=0,\n","            verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"vpAS3GSqjFo8","colab_type":"code","outputId":"80c0b78e-2207-4a53-97f0-6c161ecc7421","executionInfo":{"status":"ok","timestamp":1555124515093,"user_tz":240,"elapsed":332,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"cell_type":"code","source":["y_pred = classifier.predict(X_test)\n","\n","# acc_val = accuracy_score(y_pred, y_test)\n","# f1_val = f1_score(y_pred, y_test)\n","# recall_val = recall_score(y_pred, y_test)\n","# prec_val = precision_score(y_pred, y_test)\n","\n","print (\"Acc:\", accuracy_score(y_pred, y_test))\n","print (\"F1:\", f1_score(y_pred, y_test))\n","print (\"Recall:\", recall_score(y_pred, y_test))\n","print (\"Precision\", precision_score(y_pred, y_test))\n","\n","recalls.update({len(undersampled_data[undersampled_data.adopter == 0]) : recall_score(y_pred, y_test)})\n","f1s.update({len(undersampled_data[undersampled_data.adopter == 0]) : f1_score(y_pred, y_test)})\n","precisions.update({len(undersampled_data[undersampled_data.adopter == 0]) : precision_score(y_pred, y_test)})\n","accuracies.update({len(undersampled_data[undersampled_data.adopter == 0]) : accuracy_score(y_pred, y_test)})\n","\n","print(\"\\nF1s:\",f1s)\n","print(\"Recalls:\",recalls)\n","print(\"Precisions:\",precisions)\n","print(\"Accuracies:\",accuracies)\n","\n","print(classification_report(y_test, y_pred))\n","print(confusion_matrix(y_test, y_pred))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Acc: 0.8054818744473917\n","F1: 0.2280701754385965\n","Recall: 0.5118110236220472\n","Precision 0.14672686230248308\n","\n","F1s: {6000: 0.2280701754385965}\n","Recalls: {6000: 0.5118110236220472}\n","Precisions: {6000: 0.14672686230248308}\n","Accuracies: {6000: 0.8054818744473917}\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.97      0.89      1819\n","           1       0.51      0.15      0.23       443\n","\n","   micro avg       0.81      0.81      0.81      2262\n","   macro avg       0.67      0.56      0.56      2262\n","weighted avg       0.76      0.81      0.76      2262\n","\n","[[1757   62]\n"," [ 378   65]]\n"],"name":"stdout"}]},{"metadata":{"id":"H-AYW5gMxvqJ","colab_type":"code","outputId":"534537af-5db1-4178-f2d6-034f9edb3483","executionInfo":{"status":"ok","timestamp":1555124561011,"user_tz":240,"elapsed":2941,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# predictions on unlabelled set\n","unseen_data = pd.read_csv('https://drive.google.com/uc?export=view&id=1yVPwqGQC2gkhF2bcbue9j3184ryAJRtG')\n","# unseen_data = xgb.DMatrix(unseen_data)\n","\n","y_pred = classifier.predict(unseen_data)\n","\n","print(sum(y_pred))\n","\n","y_pred = pd.DataFrame({'Adopters': y_pred })\n","\n","# testing the model on provided test dataset\n","np.savetxt(\"rf_predictions.csv\", y_pred , delimiter=\",\")\n","from google.colab import files\n","files.download('rf_predictions.csv')\n","\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["3545\n"],"name":"stdout"}]},{"metadata":{"id":"qLwQZiGJYAvY","colab_type":"text"},"cell_type":"markdown","source":["### Feature selection\n","\n","Let's try to understand what features are the most important as the data seems to be pretty sparse and there's possibility of noise"]},{"metadata":{"id":"vCN3QuVUYASq","colab_type":"code","outputId":"f0f7b721-12e3-4543-9ffb-b8574b61a2f2","executionInfo":{"status":"ok","timestamp":1554867187467,"user_tz":240,"elapsed":9000,"user":{"displayName":"Ashok Patel","photoUrl":"","userId":"00246010457305653101"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"cell_type":"code","source":["feature_imp = pd.Series(classifier.feature_importances_).sort_values(ascending=False)\n","feature_imp"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9     0.126045\n","18    0.094097\n","4     0.073787\n","8     0.073598\n","3     0.063483\n","1     0.059883\n","0     0.056898\n","23    0.053577\n","19    0.049249\n","12    0.047444\n","14    0.043408\n","5     0.041726\n","13    0.031127\n","6     0.029272\n","7     0.026110\n","15    0.025286\n","10    0.017854\n","24    0.016657\n","11    0.016271\n","22    0.014607\n","17    0.014412\n","2     0.014258\n","16    0.008207\n","21    0.001803\n","20    0.000859\n","25    0.000080\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":24}]},{"metadata":{"id":"3_YDneVBj_GJ","colab_type":"text"},"cell_type":"markdown","source":["Looks like columns 11, 22, 16, 24, 2, 17, 21, 20, 25 aren't contributing enough. Let's try to build a classifier by removing some of these."]},{"metadata":{"id":"bsh9oYffmu38","colab_type":"code","colab":{}},"cell_type":"code","source":["# data = pd.read_csv('https://drive.google.com/uc?export=view&id=1wctM0dYDj839zp6sTlFnDgCmFspXhDuW')\n","\n","# type(data)\n","# data.columns\n","# #data.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bvAfMXtWkLog","colab_type":"code","colab":{}},"cell_type":"code","source":["# data = data.drop(data.columns[[11, 22, 16, 24, 2, 17, 21, 20, 25]], axis=1)\n","\n","# # some housekeeping for metrics\n","# recalls = {}\n","# f1s = {}\n","# precisions = {}\n","# accuracies = {}\n","\n","# # splitting original dataset into features and predictor\n","# X = data.iloc[:, data.columns != 'adopter']\n","# y = data.iloc[:, data.columns == 'adopter']\n","\n","# # splitting the original dataset for cross-validation (0.7 train, 0.3 test)\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","\n","# # fetching the indices of minority instances\n","# adopting_indices = np.array(data[data.adopter == 1].index)\n","\n","# # fetching indices of normal instances\n","# non_adopting_indices = data[data.adopter == 0].index\n","\n","# # randomly select 1540 normal instances to create a partitioned balanced dataset\n","# random_non_adopting_indices = np.random.choice(non_adopting_indices,\n","#                                             1040,\n","#                                             replace = False)\n","# random_non_adopting_indices = np.array(random_non_adopting_indices)\n","\n","# # combining both the instance groups (minority and the new random set) \n","# undersampled_indices = np.concatenate([adopting_indices, random_non_adopting_indices])\n","\n","# # creating the undersampled dataset\n","# undersampled_data = data.iloc[undersampled_indices, :]\n","\n","# # storing the features(X) and predictor class(y)\n","# X_undersample = undersampled_data.iloc[:, undersampled_data.columns != 'adopter']\n","# y_undersample = undersampled_data.iloc[:, undersampled_data.columns == 'adopter']\n","\n","# # splitting original dataset into features and predictor\n","# X = undersampled_data.iloc[:, data.columns != 'adopter']\n","# y = undersampled_data.iloc[:, data.columns == 'adopter']\n","\n","# # splitting the original dataset for cross-validation (0.7 train, 0.3 test)\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","\n","# classifier = RandomForestClassifier(random_state=0, n_jobs=-1, class_weight=\"balanced\")\n","\n","# classifier.fit(X_train, y_train)\n","\n","# y_pred = classifier.predict(X_test)\n","\n","# recalls.update({len(undersampled_data[undersampled_data.adopter == 0]) : recall_score(y_pred, y_test)})\n","# f1s.update({len(undersampled_data[undersampled_data.adopter == 0]) : f1_score(y_pred, y_test)})\n","# precisions.update({len(undersampled_data[undersampled_data.adopter == 0]) : precision_score(y_pred, y_test)})\n","# accuracies.update({len(undersampled_data[undersampled_data.adopter == 0]) : accuracy_score(y_pred, y_test)})\n","\n","# print(\"\\nF1s:\",f1s)\n","# print(\"Recalls:\",recalls)\n","# print(\"Precisions:\",precisions)\n","# print(\"Accuracies:\",accuracies)\n","\n","# print(classification_report(y_test, y_pred))\n","# print(confusion_matrix(y_test, y_pred))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0vwRXxzzrGF8","colab_type":"code","colab":{}},"cell_type":"code","source":["# # predictions on unlabelled set\n","# unseen_data = pd.read_csv('https://drive.google.com/uc?export=view&id=1yVPwqGQC2gkhF2bcbue9j3184ryAJRtG')\n","# # unseen_data = xgb.DMatrix(unseen_data)\n","# # unseen_data = unseen_data.drop(data.columns[[11, 22, 16, 24, 2, 17, 21, 20, 25]], axis=1)\n","\n","# y_pred = classifier.predict(unseen_data)\n","\n","# y_pred = pd.DataFrame({'Adopters': y_pred })"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q-t8hxfoq_Pf","colab_type":"code","colab":{}},"cell_type":"code","source":["# # testing the model on provided test dataset\n","# np.savetxt(\"predictions.csv\", y_pred , delimiter=\",\")\n","# from google.colab import files\n","# files.download('rf_predictions.csv')"],"execution_count":0,"outputs":[]}]}